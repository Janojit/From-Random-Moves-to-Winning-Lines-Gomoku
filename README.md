# ğŸ§  **From Random Moves to Winning Lines: DRL for Gomoku**

**Team Name:** NashCraft
**Members:**

* Janojit Chakraborty (B2430050)
* Radheshyam Routh (B2430053)

---

## ğŸ“Œ Project Overview

This project explores **Deep Reinforcement Learning (DRL)** in a competitive, zero-sum board game environmentâ€”**Gomoku (Five-in-a-Row)**.

The project is divided into **two major components**:

1. **Component-1:** Applying and comparing multiple DRL algorithms in a moderately difficult Gym-style environment using self-play.
2. **Component-2:** Designing a custom Gomoku environment with a graphical user interface (GUI) for interactive demonstration.

This repository contains **complete implementations**, **training scripts**, **comparative evaluation tools**, and a **visual GUI**.

---

## ğŸ—‚ï¸ Repository Structure

```
gomoku_drl/
â”‚
â”œâ”€â”€ env/                # Gym-style Gomoku environment
â”‚   â””â”€â”€ gomoku_env.py
â”‚
â”œâ”€â”€ agents/             # DRL agents
â”‚   â”œâ”€â”€ __init__.py
â”‚   â”œâ”€â”€ dqn.py
â”‚   â”œâ”€â”€ double_dqn.py
â”‚   â”œâ”€â”€ ppo.py
â”‚   â”œâ”€â”€ a2c.py
â”‚   â””â”€â”€ reinforce.py
â”‚
â”œâ”€â”€ train/              # Training scripts (self-play)
â”‚   â”œâ”€â”€ __init__.py
â”‚   â”œâ”€â”€ train_dqn.py
â”‚   â”œâ”€â”€ train_double_dqn.py
â”‚   â”œâ”€â”€ train_ppo.py
â”‚   â”œâ”€â”€ train_a2c.py
â”‚   â””â”€â”€ train_reinforce.py
â”‚
â”œâ”€â”€ eval/               # Evaluation & comparison
â”‚   â”œâ”€â”€ __init__.py
â”‚   â”œâ”€â”€ evaluate.py
â”‚   â”œâ”€â”€ compare_models.py
â”‚   â””â”€â”€ analysis.txt
â”‚
â”œâ”€â”€ gui/                # Graphical user interface
â”‚   â”œâ”€â”€ __init__.py
â”‚   â””â”€â”€ gomoku_gui.py
â”‚
â”œâ”€â”€ utils/              # Shared utilities
â”‚   â”œâ”€â”€ __init__.py
â”‚   â”œâ”€â”€ networks.py
â”‚   â”œâ”€â”€ replay_buffer.py
â”‚   â””â”€â”€ helpers.py
â”‚
â”œâ”€â”€ requirements.txt
â””â”€â”€ README.md
```

---

## âš™ï¸ Step-by-Step Instructions

---

## ğŸ”¹ STEP 1: Environment Setup

### 1. Create a Python virtual environment

```bash
python -m venv gomoku_env
source gomoku_env/bin/activate     # Linux / macOS
gomoku_env\Scripts\activate        # Windows
```

### 2. Install dependencies

```bash
pip install -r requirements.txt
```

> **Note:** GPU support is automatically enabled if CUDA is available (RTX 3050 Ti).

---

## ğŸ”¹ STEP 2: Verify the Gomoku Environment

Before training any model, verify the environment logic.

```bash
python -m env.gomoku_env
```

Ensure:

* Board initializes correctly
* Moves are applied properly
* Win detection works

---

## ğŸ”¹ STEP 3: Train DRL Agents (Component-1)

Each algorithm is trained using **self-play** in the same environment to ensure a fair comparison.

### Recommended training order:

#### 1ï¸âƒ£ DQN

```bash
python -m train.train_dqn
```

#### 2ï¸âƒ£ Double DQN

```bash
python -m train.train_double_dqn
```

#### 3ï¸âƒ£ REINFORCE

```bash
python -m train.train_reinforce
```

#### 4ï¸âƒ£ A2C

```bash
python -m train.train_a2c
```

#### 5ï¸âƒ£ PPO (Main Algorithm)

```bash
python -m train.train_ppo
```

ğŸ“Œ **Output:**
Trained models are saved as `.pth` files in the project root.

---

## ğŸ”¹ STEP 4: Evaluate and Compare Models

### 1. Run evaluation matches

```bash
python -m eval.evaluate
```

This script:

* Runs fixed evaluation episodes
* Measures wins, losses, and draws
* Saves results to disk

### 2. Generate comparison plots

```bash
python -m eval.compare_models
```

ğŸ“Š **Generated Outputs:**

* `win_count.png`
* `loss_count.png`
* `draw_count.png`

### 3. Play Tournament

```
python -m eval.tournament
```

This file explains:

* Training stability
* Algorithmic strengths and weaknesses
* Self-play behavior differences

---

## ğŸ”¹ STEP 5: Launch the GUI (Component-2)

Run the interactive Gomoku interface:

```bash
python -m gui.gomoku_gui
```

### ğŸ® Available Modes

* **Human vs Human**
* **Human vs Agent**
* **Agent vs Agent**

### ğŸ¤– AI Selection

* DQN
* Double-DQN
* PPO
* A2C
* REINFORCE

The GUI visually displays:

* Game board
* Player turns
* Winning condition

---

## ğŸ§ª Algorithms Used

| Algorithm  | Type         | Purpose              |
| ---------- | ------------ | -------------------- |
| DQN        | Value-based  | Baseline             |
| Double-DQN | Value-based  | Reduced bias         |
| REINFORCE  | Policy-based | Monte-Carlo learning |
| A2C        | Actor-Critic | Faster convergence   |
| PPO        | Policy-based | Stable & robust      |

---

## ğŸ“ˆ Evaluation Metrics

* Win rate
* Loss rate
* Draw frequency
* Training stability
* Convergence behavior

---

## ğŸ¯ Key Takeaways

* Value-based methods struggle with non-stationary self-play.
* Policy-gradient methods (PPO, A2C) show superior stability.
* Strategic behaviors emerge without handcrafted heuristics.
* Custom environment + GUI adds originality beyond benchmarks.

---

## ğŸ Conclusion

This project demonstrates how **Deep Reinforcement Learning**, combined with **self-play**, can transform random actions into strategic decision-making in competitive board games. The modular design allows easy extension to larger boards or additional algorithms.

---

## ğŸ“¬ Contact

For questions or collaboration:

* **Team NashCraft**
* MSc Big Data Analytics, RKMVERI

---
Just tell me.
